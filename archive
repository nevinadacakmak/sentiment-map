#objective
#Building a sentiment analysis platform that tracks and visualizes public mood across Canadian provinces using Reddit, local news.
#• Engineered data pipeline to collect, clean, and process natural language content; stored structured data in MongoDB.
#• Applied NLP and unsupervised clustering to detect trends, keywords, and sentiment shifts; visualized results with a Plotly dashboard

#to do
#implement reddit api
#finish the project please. without gpt, learn the design decisions and system decisions
#put it in your website too

import pandas as pd #data analysis
import numpy as np #md arrays
import sklearn as sk #algorithms
import nltk #natural language processing
import pymongo #MongoDB
import requests #HTTP requests
from bs4 import BeautifulSoup #web scraping
import plotly.express as px #visualization

import requests
from bs4 import BeautifulSoup

#engineer a data pipeline

    #collect data
''' Note: Since Reddit's User Agreement requires API usage for scraping, we will leave this part commented out and use the Reddit API instead.
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}


def get_reddit_data(subreddit):
    #old.reddit.com has simple html structure
    url = f'https://old.reddit.com/r/{subreddit}/'
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    posts = soup.find_all('div', class_='thing')
    
    #scrape posts
    for post in posts:
        title = post.find('h3').text
        url = post.find('a', class_='title')['href']
        content = get_post_content(url)
        posts.append({
            'title': title,
            'url': url,
            'content': content
        })
    return posts

def get_post_content(url):
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    content = soup.find('div', class_='usertext-body').text
    return content

#example for now
subreddit = 'toronto'
posts = get_reddit_data(subreddit)

import json
with open(f'{subreddit}_data.json', 'w') as f:
    json.dump(posts, f, indent=4)
'''
    

    #process natural language content - use VADER? or nltk

    #store data in MongoDB

    #do orchestration - apache kafka?

#apply NLP and unsupervised clustering

#visualize results with Plotly dashboard



